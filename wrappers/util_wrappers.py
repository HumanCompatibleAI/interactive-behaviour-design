import multiprocessing
import os
import pickle
import queue
import tempfile
import time
from collections import deque
from enum import Enum
from functools import partial
from multiprocessing import Queue

from gym.wrappers import TimeLimit
from os import path as osp
from threading import Thread, Lock
from typing import Dict

import cv2
import easy_tf_log
import numpy as np
from gym.core import ObservationWrapper, Wrapper

import global_constants
from baselines import logger
from classifier_collection import ClassifierCollection
from drlhp.reward_predictor import RewardPredictor
from subproc_vec_env_custom import CustomSubprocVecEnv, CustomVecEnvWrapper
from utils import unwrap_to, EnvState, TimerContext
from wrappers.state_boundary_wrapper import StateBoundaryWrapper


class DrawClassifierPredictionWrapper(ObservationWrapper):
    classifier: ClassifierCollection

    def __init__(self, env, classifier):
        ObservationWrapper.__init__(self, env)
        self.classifier = classifier

    def observation(self, obs):
        obs = np.copy(obs)
        obs_width = obs.shape[1]
        labels = self.classifier.get_labels()
        for label_n, label_name in enumerate(labels):
            pred = self.classifier.predict(label_name, [obs])[0]
            if pred == 0:
                color = (0, 0, 0, 0)
            elif pred == 1:
                color = (255, 255, 255, 255)
            else:
                raise Exception("Bad pred: '{}'".format(pred))
            x = int(label_n * obs_width / len(labels))
            cv2.putText(obs,
                        label_name,
                        org=(5 + x, 20),
                        fontFace=cv2.FONT_HERSHEY_PLAIN,
                        fontScale=1.0,
                        color=color,
                        thickness=2)
        return obs


class VecLogRewards(CustomVecEnvWrapper):
    def __init__(self, venv, log_dir, postfix=None):
        super().__init__(venv)
        self.episode_reward_sum = 0
        self.logger = easy_tf_log.Logger()
        self.logger.set_log_dir(log_dir)
        self.log_key = 'env_train/episode_reward_sum_vec'
        if postfix is not None:
            self.log_key += postfix

    def step_wait(self):
        obses, rewards, dones, infos = self.venv.step_wait()
        self.episode_reward_sum += rewards[0]
        if dones[0]:
            self.logger.logkv(self.log_key, self.episode_reward_sum)
            self.episode_reward_sum = 0
        return obses, rewards, dones, infos

    def reset(self):
        return self.venv.reset()


class SaveEpisodeStats(Wrapper):
    def __init__(self, env, log_dir=None, suffix=None, stdout=False):
        Wrapper.__init__(self, env)
        self.stdout = stdout
        if log_dir is None:
            self.logger = None
        else:
            self.logger = easy_tf_log.Logger(log_dir)
        self.suffix = ('' if suffix is None else suffix)
        self.stats_envs = None
        self.stats = {}
        self.set_env(env)

    def set_env(self, env):
        self.env = env
        self.stats_envs = []
        while True:
            if isinstance(env, CollectEpisodeStats):
                self.stats_envs.append(env)
            try:
                env = env.env
            except:
                break

    def step(self, action):
        return self.env.step(action)

    def reset(self):
        obs = self.env.reset()
        self.stats = {}
        for stats_env in self.stats_envs:
            for key, value in stats_env.last_stats.items():
                self.stats[key] = value
                key_with_suffix = f'env{self.suffix}/' + key
                if self.logger:
                    self.logger.logkv(key_with_suffix, value)
                if self.stdout:
                    print(key_with_suffix + ':', value)
            stats_env.last_stats = {}
        return obs


class LogDoneInfo(Wrapper):
    def __init__(self, env):
        Wrapper.__init__(self, env)
        self.logger = easy_tf_log.Logger()
        self.logger.set_log_dir(osp.join(logger.get_dir(), "env2"))

    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        if done:
            for key, value in info.items():
                self.logger.logkv('env/' + key, value)
        return obs, reward, done, info


class CollectEpisodeStats(Wrapper):
    """
    Save per-episode rewards and episode lengths.
    """

    def __init__(self, env, suffix='', rewards_only=False):
        Wrapper.__init__(self, env)
        self.suffix = suffix
        self.rewards_only = rewards_only
        self.episode_n = -1
        self.episode_rewards = self.episode_length_steps = None
        self.last_stats = {}

    def reset(self):
        self.last_stats = {}
        if self.episode_n >= 0:
            self.last_stats['reward_sum' + self.suffix] = sum(self.episode_rewards)
            if not self.rewards_only:
                self.last_stats['episode_n' + self.suffix] = self.episode_n
                self.last_stats['episode_length_steps' + self.suffix] = self.episode_length_steps

        self.episode_n += 1
        self.episode_rewards = []
        self.episode_length_steps = 0

        return self.env.reset()

    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        self.episode_rewards.append(reward)
        self.episode_length_steps += 1
        return obs, reward, done, info


class SeaquestStatsWrapper(CollectEpisodeStats):
    def __init__(self, env):
        CollectEpisodeStats.__init__(self, env)
        self.last_n_divers = None
        self.n_diver_pickups = None

    def reset(self):
        self.last_stats = {}
        if self.n_diver_pickups is not None:
            self.last_stats['n_diver_pickups'] = self.n_diver_pickups
        self.last_n_divers = 0
        self.n_diver_pickups = 0
        return self.env.reset()

    def get_n_divers_picked_up(self):
        return self.env.unwrapped.ale.getRAM()[62]

    def step(self, action):
        obs, reward, done, info = self.env.step(action)

        cur_n_divers = self.get_n_divers_picked_up()
        if cur_n_divers > self.last_n_divers:
            self.n_diver_pickups += (cur_n_divers - self.last_n_divers)
        self.last_n_divers = cur_n_divers

        return obs, reward, done, info


class ResetMode(Enum):
    USE_ENV_RESET = 0
    FROM_STATE_CACHE = 1


class QueueEndpoint(Enum):
    DEMONSTRATIONS = 0
    TRAINING = 1


class ResetStateCache:
    def __init__(self, log_dir: str):
        self.log_dir = log_dir
        self.logger = easy_tf_log.Logger()
        self.logger.set_log_dir(os.path.join(log_dir, 'reset_states'))
        self.in_queues = dict()  # type: Dict[QueueEndpoint, multiprocessing.Queue]
        self.out_queues = dict()  # type: Dict[QueueEndpoint, multiprocessing.Queue]
        self.receive_to_pool = dict()  # type: Dict[QueueEndpoint, str]
        self.serve_from_pool = dict()  # type: Dict[QueueEndpoint, str]
        self.pools = dict()  # type: Dict[str, deque]
        self.lock = Lock()
        for endpoint in [QueueEndpoint.DEMONSTRATIONS, QueueEndpoint.TRAINING]:
            # We limit the size so that we don't get a massive backlog
            self.in_queues[endpoint] = multiprocessing.Queue(maxsize=50)
            self.out_queues[endpoint] = multiprocessing.Queue(maxsize=1)
            self.receive_to_pool[endpoint] = None
            self.serve_from_pool[endpoint] = None
        self.queue_from_demonstrations = self.in_queues[QueueEndpoint.DEMONSTRATIONS]
        self.queue_from_training = self.in_queues[QueueEndpoint.TRAINING]
        self.queue_to_demonstrations = self.out_queues[QueueEndpoint.DEMONSTRATIONS]
        self.queue_to_training = self.out_queues[QueueEndpoint.TRAINING]

    def start_saver_receiver(self):
        for endpoint in [QueueEndpoint.DEMONSTRATIONS, QueueEndpoint.TRAINING]:
            Thread(target=self.in_queue_receiver, args=[endpoint]).start()
            Thread(target=self.out_queue_server, args=[endpoint]).start()

    def in_queue_receiver(self, endpoint: QueueEndpoint):
        queue = self.in_queues[endpoint]
        while True:
            reset_state = queue.get()
            pool_name = self.receive_to_pool[endpoint]
            if pool_name is None:
                continue
            if pool_name not in self.pools:
                print("Warning: pool '{}' not found".format(pool_name))
                continue
            self.lock.acquire()
            self.pools[pool_name].append(reset_state)
            self.lock.release()
            self.logger.logkv(f'reset_states/{pool_name}_len', len(self.pools[pool_name]))

    def out_queue_server(self, target: QueueEndpoint):
        queue = self.out_queues[target]
        while True:
            pool_name = self.serve_from_pool[target]
            if pool_name is None:
                time.sleep(5.0)
                continue
            if pool_name not in self.pools:
                print("Warning: pool '{}' not found".format(pool_name))
                time.sleep(5.0)
                continue
            pool = self.pools[pool_name]
            if len(pool) == 0:
                print("Warning: pool '{}' is empty".format(pool_name))
                time.sleep(5.0)
                continue
            n = np.random.choice(len(pool))
            reset_state = pool[n]
            queue.put(reset_state)
            msg_str = "Supplied reset state {} to {} from pool '{}'".format(
                n, str(target), pool_name)
            print(msg_str)

    def add_pool(self, name, max_len):
        if name in self.pools:
            raise Exception("Pool '{}' already exists".format(name))
        self.pools[name] = deque(maxlen=max_len)

    def load_dir(self, dir):
        path = os.path.join(dir, 'reset_states.pkl')
        if not os.path.exists(path):
            print("Warning: reset_states.pkl not found")
            return
        with open(path, 'rb') as f:
            self.pools = pickle.load(f)
        print("Loaded reset states from '{}'".format(path))

    def save(self):
        self.lock.acquire()
        fd, temp_file = tempfile.mkstemp(dir=self.log_dir)
        with open(temp_file, 'wb') as f:
            pickle.dump(self.pools, f)
        os.close(fd)
        save_path = os.path.join(self.log_dir, 'reset_states.pkl')
        os.rename(temp_file, save_path)
        self.lock.release()


class StoredResetsWrapper(Wrapper):

    def __init__(self, env, reset_mode_value: multiprocessing.Value, reset_state_queue: Queue):
        Wrapper.__init__(self, env)
        self.reset_state_queue = reset_state_queue
        self.mode = reset_mode_value
        self.env_boundary = unwrap_to(self.env, StateBoundaryWrapper)

    def step(self, action):
        return self.env.step(action)

    def reset(self):
        if self.mode.value == ResetMode.USE_ENV_RESET.value:
            return self.env.reset()
        elif self.mode.value == ResetMode.FROM_STATE_CACHE.value:
            reset_state = self.get_reset_state()
            self.env_boundary.env = reset_state.env
            return reset_state.obs
        else:
            print("Warning: unknown reset mode '{}'; using env reset".format(self.mode.value))
            return self.env.reset()

    def get_reset_state(self):
        while True:
            try:
                reset_state = self.reset_state_queue.get(block=True,
                                                         timeout=0.1)  # type: EnvState
                break
            except queue.Empty:
                print("Waiting for reset state...")
                time.sleep(1.0)
        return reset_state


class SaveMidStateWrapper(Wrapper):
    SAVE_EVERY_NTH_STEP = 10

    def __init__(self, env, state_save_queue: multiprocessing.Queue, verbose=True):
        Wrapper.__init__(self, env)
        self.state_save_queue = state_save_queue
        self.verbose = verbose
        self.states = None
        self.step_n = 1
        self.states = []
        self.env_boundary = unwrap_to(self.env, StateBoundaryWrapper)

    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        # Make sure we save at least one step per episode
        if self.step_n == 1 or self.step_n % SaveMidStateWrapper.SAVE_EVERY_NTH_STEP == 0:
            reset_state = EnvState(self.env_boundary.env, obs, done, self.step_n)
            self.states.append(reset_state)
        self.step_n += 1
        return obs, reward, done, info

    def filter_states(self):
        if 'Seaquest' in self.env.spec.id:
            # Filter out states which are too close to the end
            # (after the submarine has already died)
            self.states = [state for state in self.states if state.step_n < self.step_n - 25]

    def reset(self):
        self.filter_states()

        if self.states:
            state_n = np.random.randint(low=0, high=len(self.states))
            state = self.states[state_n]
            try:
                self.state_save_queue.put(state, block=False)
                if self.verbose:
                    print("Saved reset state from step", state.step_n)
            except queue.Full:
                pass

        self.states = []
        self.step_n = 1

        return self.env.reset()


class OneLifeWrapper(Wrapper):

    def __init__(self, env):
        Wrapper.__init__(self, env)
        self.last_lives = None

    def reset(self):
        self.last_lives = self.env.unwrapped.ale.lives()
        return self.env.reset()

    def step(self, a):
        obs, reward, done, info = self.env.step(a)
        if self.env.unwrapped.ale.lives() < self.last_lives:
            done = True
        self.last_lives = self.env.unwrapped.ale.lives()
        return obs, reward, done, info


class SaveEpisodeObs(Wrapper):
    def __init__(self, env, queue):
        Wrapper.__init__(self, env)
        self.queue = queue
        self.obses = []
        self.episode_n = -1

    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        self.obses.append(obs)
        return obs, reward, done, info

    def send_frames(self):
        if self.obses:
            self.queue.put((self.episode_n, self.obses))

    def reset(self):
        self.send_frames()
        self.episode_n += 1
        obs = self.env.reset()
        self.obses = [obs]
        return obs


class DummyRender(Wrapper):
    def __init__(self, env):
        Wrapper.__init__(self, env)

    def render(self, mode='rgb_array', **kwargs):
        assert mode == 'rgb_array'
        return np.zeros((2, 2, 3), dtype=np.uint8)

    def step(self, action):
        return self.env.step(action)

    def reset(self):
        return self.env.reset()


class VecSaveSegments(CustomVecEnvWrapper):
    def __init__(self, venv, segment_queue: multiprocessing.Queue):
        assert isinstance(venv, CustomSubprocVecEnv)
        super().__init__(venv)
        self.queue = segment_queue
        self.segment_frames = [None] * self.num_envs
        self.segment_obses = [None] * self.num_envs
        self.segment_rewards = [None] * self.num_envs
        for n in range(self.num_envs):
            self._reset_segment(n)

    def _reset_segment(self, n):
        self.segment_frames[n] = []
        self.segment_obses[n] = []
        self.segment_rewards[n] = []

    def _pad_segment(self, n):
        while len(self.segment_obses[n]) < global_constants.FRAMES_PER_SEGMENT:
            self.segment_frames[n].append(self.segment_frames[n][-1])
            self.segment_obses[n].append(self.segment_obses[n][-1])
            self.segment_rewards[n].append(self.segment_rewards[n][-1])

    def step_wait(self):
        obses, rewards, dones, infos = self.venv.step_wait()
        frames = self.venv.get_images()

        for n in range(self.num_envs):
            self.segment_frames[n].append(frames[n])
            self.segment_obses[n].append(np.copy(obses[n]))
            self.segment_rewards[n].append(rewards[n])

            segment_full = len(self.segment_obses[n]) == global_constants.FRAMES_PER_SEGMENT
            # We could get unlucky and get a 'done' just after we've reset the segment
            segment_done = dones[n] and len(self.segment_obses[n]) > 0
            if segment_full or segment_done:
                self._pad_segment(n)
                tuple = (self.segment_obses[n], self.segment_rewards[n], self.segment_frames[n])
                try:
                    self.queue.put(tuple, block=False)
                except queue.Full:
                    pass
                self._reset_segment(n)
        return obses, rewards, dones, infos

    def reset_one_env(self, env_n):
        # Shouldn't we have seen 'done' before reset, making this call unnecessary?
        # Not necessarily: if we've just switched policy, that policy is going to want to reset the environment,
        # even if the previous policy hadn't finished the episode
        self._reset_segment(env_n)
        return super().reset_one_env(env_n)


class SaveSegments(Wrapper):
    def __init__(self, env, segment_queue: multiprocessing.Queue):
        Wrapper.__init__(self, env)
        self.queue = segment_queue
        self.segment_frames = None
        self.segment_obses = None
        self.segment_rewards = None
        self._reset_segment()

    def _reset_segment(self):
        self.segment_frames = []
        self.segment_obses = []
        self.segment_rewards = []

    def _pad_segment(self):
        while len(self.segment_obses) < global_constants.FRAMES_PER_SEGMENT:
            self.segment_frames.append(self.segment_frames[-1])
            self.segment_obses.append(self.segment_obses[-1])
            self.segment_rewards.append(self.segment_rewards[-1])

    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        self.segment_frames.append(self.env.render(mode='rgb_array'))
        self.segment_obses.append(np.copy(obs))
        self.segment_rewards.append(reward)
        if done or len(self.segment_obses) == global_constants.FRAMES_PER_SEGMENT:
            self._pad_segment()
            tuple = (self.segment_obses, self.segment_rewards, self.segment_frames)
            try:
                self.queue.put(tuple, block=False)
            except queue.Full:
                pass
            self._reset_segment()
        return obs, reward, done, info

    def reset(self):
        if self.segment_frames:
            # We should have seen 'done' during 'step'
            raise RuntimeError("segment_frames not empty on env reset")
        return self.env.reset()


class EpisodeLengthLimitWrapper(Wrapper):
    env: TimeLimit

    def __init__(self, env, max_episode_steps_value: multiprocessing.Value):
        Wrapper.__init__(self, env)
        self.max_episode_steps_value = max_episode_steps_value
        self.n_steps = 0

    def reset(self):
        self.n_steps = 0
        return self.env.reset()

    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        self.n_steps += 1
        if self.n_steps > self.max_episode_steps_value.value:
            done = True
        return obs, reward, done, info


class Profile(Wrapper):
    @staticmethod
    def _time_step(step_fn, action):
        with TimerContext(str(step_fn)):
            obs, reward, done, info = step_fn(action)
        return obs, reward, done, info

    @staticmethod
    def _print_sep_and_step(step_fn, action):
        print()
        return step_fn(action)

    def __init__(self, env):
        Wrapper.__init__(self, env)
        while True:
            env.step = partial(self._time_step, env.step)
            try:
                env = env.env
            except:
                env.step = partial(self._print_sep_and_step, env.step)
                break

    def reset(self):
        return self.env.reset()


class RepeatActions(Wrapper):
    def __init__(self, env, repeat_n):
        Wrapper.__init__(self, env)
        assert repeat_n > 0
        self.repeat_n = repeat_n

    def step(self, action):
        rewards = []
        for _ in range(self.repeat_n):
            obs, reward, done, info = self.env.step(action)
            rewards.append(reward)
            if done:
                break
        return obs, sum(rewards), done, info

    def reset(self):
        return self.env.reset()
